# data_extract.py - æ•°æ®å¤„ç†è½¦é—´ (V8 - å®Œæ•´æ”¯æŒCSVçš„4ç§è¯†åˆ«æ–¹æ³•)

from pathlib import Path
import pandas as pd
import json
from decimal import Decimal

# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
def find_field_from_aliases(columns, aliases):
    for alias in aliases:
        if alias in columns:
            return alias
    return None

# --- é«˜çº§è§£æå·¥å…· (æ–°å¢ä¸€ä¸ª + ä¼˜åŒ–ä¸€ä¸ª) ---

def _find_subtable_start_row(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> int:
    """
    (V3 - "å¤šè·¯æ ‡"ç‰ˆ) 
    åœ¨å·¥ä½œè¡¨ä¸­ï¼Œæ ¹æ®ä¸€ä¸ª"è·¯æ ‡"åˆ«ååˆ—è¡¨ï¼Œä¾æ¬¡å°è¯•å¯»æ‰¾ï¼Œå¹¶è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„å­è¡¨çš„è¡¨å¤´è¡Œå·ã€‚
    """
    # --- æ ¸å¿ƒå‡çº§ï¼šéå†"è·¯æ ‡"åˆ—è¡¨ ---
    for header in section_header_aliases:
        print(f"  - å°è¯•å®šä½è·¯æ ‡: '{header}'...")
        matches = sheet_df.stack() == header
        true_matches = matches[matches].index

        if len(true_matches) > 0:
            # åªè¦æ‰¾åˆ°äº†ç¬¬ä¸€ä¸ªï¼Œå°±ç«‹åˆ»è¿”å›ç»“æœï¼Œä¸å†ç»§ç»­å¯»æ‰¾ï¼
            header_row_index = true_matches[0][0]
            print(f"    âœ… æˆåŠŸï¼åœ¨ç¬¬ {header_row_index} è¡Œæ‰¾åˆ°äº†ç²¾ç¡®åŒ¹é…çš„è·¯æ ‡ã€‚")
            return header_row_index + header_offset
    
    # å¦‚æœæŠŠåˆ—è¡¨é‡Œæ‰€æœ‰è·¯æ ‡éƒ½è¯•å®Œäº†ï¼Œè¿˜æ˜¯æ²¡æ‰¾åˆ°
    print(f"  - ğŸŸ¡ è­¦å‘Š: åœ¨å·¥ä½œè¡¨ä¸­æœªèƒ½æ‰¾åˆ°ä»»ä½•ä¸€ä¸ªæŒ‡å®šçš„è·¯æ ‡: {section_header_aliases}ã€‚")
    return None

def _parse_tabular_subtable(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> pd.DataFrame:
    """
    (V2 - æ™ºèƒ½ç»“æŸç‰ˆ) 
    è§£æä¸€ä¸ªæ ‡å‡†çš„å¤šè¡Œå­è¡¨ã€‚èƒ½å¤Ÿé€šè¿‡å¯»æ‰¾å…¨ç©ºè¡Œæ¥æ™ºèƒ½åˆ¤æ–­å­è¡¨çš„ç»“æŸä½ç½®ã€‚
    """
    header_row = _find_subtable_start_row(sheet_df, section_header_aliases, header_offset)
    if header_row is None:
        return pd.DataFrame()
    
    # --- æ ¸å¿ƒå‡çº§ï¼šæ™ºèƒ½åˆ¤æ–­ç»“æŸä½ç½® ---
    potential_data = sheet_df.iloc[header_row + 1:]
    
    try:
        first_empty_row_index = potential_data.isnull().all(axis=1).eq(True).idxmax()
        if potential_data.loc[first_empty_row_index].isnull().all():
             end_index = first_empty_row_index
        else:
             end_index = None
    except ValueError:
        end_index = None

    if end_index is not None:
        actual_data = potential_data.loc[:end_index-1]
    else:
        actual_data = potential_data
        
    sub_table = actual_data.copy()
    sub_table.columns = sheet_df.iloc[header_row].values
    sub_table.reset_index(drop=True, inplace=True)
    sub_table.dropna(how='all', inplace=True)
    
    return sub_table

def _parse_form_subtable(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> dict:
    header_row = _find_subtable_start_row(sheet_df, section_header_aliases, header_offset)
    if header_row is None: 
        return {}
    
    data_row_index = header_row + 1
    
    form_data = pd.Series(
        sheet_df.iloc[data_row_index].values,
        index=sheet_df.iloc[header_row].values
    ).to_dict()
    
    return form_data

def _parse_merged_key_value(sheet_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    merged_cols = config['merged_columns']
    key_col = config['key_column']
    value_col = config['value_column']
    
    sheet_df[merged_cols] = sheet_df[merged_cols].fillna(method='ffill')
    sheet_df.dropna(subset=[merged_cols[0]], inplace=True)
    
    output_records = []
    for group_keys, group_df in sheet_df.groupby(merged_cols):
        record = dict(zip(merged_cols, group_keys))
        extra_data = group_df[[key_col, value_col]].dropna(subset=[key_col]).set_index(key_col).to_dict()[value_col]
        
        # print(f"\n--- [è¯Šæ–­æ¢é’ˆ#1A] è§£æå™¨ä¸ºè®¾å¤‡ '{record.get('Device Name è®¾å¤‡åç§°')}' ç”Ÿæˆçš„ extra_data ---")
        # print(extra_data)
        
        record['extra_data'] = extra_data
        output_records.append(record)
        
    final_df = pd.DataFrame(output_records)
    
    # print(f"\n--- [è¯Šæ–­æ¢é’ˆ#1B] è§£æå™¨æœ€ç»ˆç”Ÿæˆçš„DataFrameé¢„è§ˆ (åº”åŒ…å«extra_dataåˆ—) ---")
    # print(final_df.head().to_string())
    
    return final_df

# --- CSVç¼–ç æ£€æµ‹å‡½æ•° ---
def _detect_csv_encoding(file_path: Path) -> str:
    """
    æ£€æµ‹CSVæ–‡ä»¶çš„ç¼–ç æ ¼å¼
    """
    # å¸¸è§çš„ç¼–ç åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1', 'cp1252']
    
    for encoding in encodings:
        try:
            # å°è¯•è¯»å–å‰å‡ è¡Œæ¥éªŒè¯ç¼–ç 
            with open(file_path, 'r', encoding=encoding) as f:
                f.read(1000)  # è¯»å–å‰1000ä¸ªå­—ç¬¦æµ‹è¯•
            return encoding
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤ç¼–ç 
    return 'utf-8'

# --- CSVä¸“ç”¨è§£æå‡½æ•° ---

def _parse_form_subtable_csv(csv_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> dict:
    """
    CSVç‰ˆæœ¬çš„è¡¨å•è§£æå‡½æ•°
    """
    header_row = _find_subtable_start_row(csv_df, section_header_aliases, header_offset)
    if header_row is None: 
        return {}
    
    data_row_index = header_row + 1
    if data_row_index >= len(csv_df):
        return {}
    
    form_data = pd.Series(
        csv_df.iloc[data_row_index].values,
        index=csv_df.iloc[header_row].values
    ).to_dict()
    
    return form_data

def _parse_tabular_subtable_csv(csv_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> pd.DataFrame:
    """
    CSVç‰ˆæœ¬çš„åŠ¨æ€å­è¡¨è§£æå‡½æ•°
    """
    header_row = _find_subtable_start_row(csv_df, section_header_aliases, header_offset)
    if header_row is None:
        return pd.DataFrame()
    
    potential_data = csv_df.iloc[header_row + 1:]
    
    try:
        first_empty_row_index = potential_data.isnull().all(axis=1).eq(True).idxmax()
        if potential_data.loc[first_empty_row_index].isnull().all():
             end_index = first_empty_row_index
        else:
             end_index = None
    except ValueError:
        end_index = None

    if end_index is not None:
        actual_data = potential_data.loc[:end_index-1]
    else:
        actual_data = potential_data
        
    sub_table = actual_data.copy()
    sub_table.columns = csv_df.iloc[header_row].values
    sub_table.reset_index(drop=True, inplace=True)
    sub_table.dropna(how='all', inplace=True)
    
    return sub_table

def _parse_merged_key_value_csv(csv_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    CSVç‰ˆæœ¬çš„å¤æ‚ä¸‰ç»´è¡¨è§£æå‡½æ•°
    """
    merged_cols = config['merged_columns']
    key_col = config['key_column']
    value_col = config['value_column']
    
    # å…ˆè®¾ç½®æ­£ç¡®çš„åˆ—å
    if 'header_row' in config:
        header_row = config['header_row'] - 1
        csv_df.columns = csv_df.iloc[header_row].values
        csv_df = csv_df.iloc[header_row + 1:].reset_index(drop=True)
    
    csv_df[merged_cols] = csv_df[merged_cols].fillna(method='ffill')
    csv_df.dropna(subset=[merged_cols[0]], inplace=True)
    
    output_records = []
    for group_keys, group_df in csv_df.groupby(merged_cols):
        record = dict(zip(merged_cols, group_keys))
        extra_data = group_df[[key_col, value_col]].dropna(subset=[key_col]).set_index(key_col).to_dict()[value_col]
        
        # print(f"\n--- [CSVè¯Šæ–­æ¢é’ˆ#1A] è§£æå™¨ä¸ºè®¾å¤‡ '{record.get('Device Name è®¾å¤‡åç§°')}' ç”Ÿæˆçš„ extra_data ---")
        # print(extra_data)
        
        record['extra_data'] = extra_data
        output_records.append(record)
        
    final_df = pd.DataFrame(output_records)
    
    # print(f"\n--- [CSVè¯Šæ–­æ¢é’ˆ#1B] è§£æå™¨æœ€ç»ˆç”Ÿæˆçš„DataFrameé¢„è§ˆ (åº”åŒ…å«extra_dataåˆ—) ---")
    # print(final_df.head().to_string())
    
    return final_df

# --- æ ¸å¿ƒ"å–è´§å‘˜"å‡½æ•° (å‡çº§ç‰ˆ) ---
def extract_data_from_sources(excel_path: Path, sources_config: list) -> dict:
    """
    (V9ç‰ˆ - ä¸¥æ ¼éªŒè¯ç‰ˆ)
    èƒ½å¤Ÿæ ¹æ®æ–‡ä»¶åç¼€åï¼Œæ™ºèƒ½é€‰æ‹©Excelæˆ–CSVçš„è§£æç­–ç•¥ã€‚
    æ–°å¢ï¼šä¸¥æ ¼éªŒè¯æ‰€æœ‰sourceséƒ½å¿…é¡»æˆåŠŸæå–æ•°æ®ï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸ã€‚
    """
    extracted_data = {}
    failed_sources = []  # è®°å½•å¤±è´¥çš„æ•°æ®æº
    success_sources = []  # è®°å½•æˆåŠŸçš„æ•°æ®æº
    available_sheets = []  # åˆå§‹åŒ–å¯ç”¨å·¥ä½œè¡¨åˆ—è¡¨ï¼Œç¡®ä¿åœ¨æ•´ä¸ªå‡½æ•°ä¸­éƒ½å¯ç”¨
    file_extension = excel_path.suffix.lower()

    print(f"    ğŸ“‹ éœ€è¦æå– {len(sources_config)} ä¸ªæ•°æ®æº")
    for source in sources_config:
        source_id = source.get('source_id', 'unknown')
        print(f"    - {source_id}: {source.get('worksheet_name', 'CSVæ•°æ®')}")

    # --- âœ¨ æ ¸å¿ƒæ”¹é€ ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„è¯»å–ç­–ç•¥ âœ¨ ---
    if file_extension == '.csv':
        # --- CSV æ–‡ä»¶å¤„ç†é€»è¾‘ (æ”¯æŒ4ç§è¯†åˆ«æ–¹æ³•å’Œç¼–ç æ£€æµ‹) ---
        print(f"    ğŸ“„ è§£æCSVæ–‡ä»¶: {excel_path.name}")
        
        # é¦–å…ˆæ£€æµ‹æ–‡ä»¶ç¼–ç 
        detected_encoding = _detect_csv_encoding(excel_path)
        print(f"    ğŸ” æ–‡ä»¶ç¼–ç : {detected_encoding}")
        
        try:
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–æ•´ä¸ªCSVæ–‡ä»¶ä½œä¸ºåŸå§‹æ•°æ®
            csv_df = pd.read_csv(excel_path, header=None, encoding=detected_encoding)  # ä¸è®¾ç½®headerï¼Œä¿æŒåŸå§‹ç»“æ„
            
            # éå†é…ç½®ä¸­çš„æ¯ä¸ªæ•°æ®æºï¼Œæ ¹æ®layoutç±»å‹è¿›è¡Œå¤„ç†
            for source in sources_config:
                source_id = source['source_id']
                layout = source.get('data_layout', 'tabular')
                print(f"    ğŸ“‹ å¤„ç†æ•°æ®æº: '{source_id}' ({layout})")
                
                try:
                    # --- CSVè°ƒåº¦ä¸­å¿ƒï¼šæ ¹æ®å¸ƒå±€ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³• ---
                    if layout == 'tabular':
                        # æ ‡å‡†è¡¨æ ¼ï¼šä½¿ç”¨æŒ‡å®šçš„header_row
                        header_row = source.get('header_row', 1) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
                        df = csv_df.copy()
                        if header_row < len(csv_df):
                            df.columns = csv_df.iloc[header_row].values  # è®¾ç½®è¡¨å¤´
                            df = df.iloc[header_row + 1:].reset_index(drop=True)  # å»æ‰è¡¨å¤´è¡Œ
                            df.dropna(how='all', inplace=True)  # æ¸…ç†ç©ºè¡Œ
                        else:
                            df = pd.DataFrame()
                        
                    elif layout == 'form_layout':
                        # è¡¨å•å¸ƒå±€ï¼šå¯»æ‰¾ç‰¹å®šæ ‡é¢˜å¹¶è§£æé”®å€¼å¯¹
                        header_aliases = source.get('section_header_aliases', [])
                        header_offset = source.get('header_offset', 1)
                        df = _parse_form_subtable_csv(csv_df, header_aliases, header_offset)
                        
                    elif layout == 'find_subtable_by_header':
                        # åŠ¨æ€å­è¡¨ï¼šå¯»æ‰¾ç‰¹å®šæ ‡é¢˜ä¸‹çš„è¡¨æ ¼æ•°æ®
                        header_aliases = source.get('section_header_aliases', [])
                        header_offset = source.get('header_offset', 1)
                        df = _parse_tabular_subtable_csv(csv_df, header_aliases, header_offset)
                        
                    elif layout == 'merged_key_value':
                        # å¤æ‚ä¸‰ç»´è¡¨ï¼šå¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œé”®å€¼å¯¹
                        df = _parse_merged_key_value_csv(csv_df, source)
                        
                    else:
                        print(f"  - ğŸŸ¡ è­¦å‘Š: æœªçŸ¥çš„CSVå¸ƒå±€ç±»å‹ '{layout}'ï¼Œä½¿ç”¨é»˜è®¤tabularå¤„ç†ã€‚")
                        # é»˜è®¤æŒ‰æ ‡å‡†è¡¨æ ¼å¤„ç†
                        header_row = source.get('header_row', 1) - 1
                        df = csv_df.copy()
                        if header_row < len(csv_df):
                            df.columns = csv_df.iloc[header_row].values
                            df = df.iloc[header_row + 1:].reset_index(drop=True)
                            df.dropna(how='all', inplace=True)
                        else:
                            df = pd.DataFrame()
                    
                    # âœ… æ™ºèƒ½éªŒè¯ï¼šåŒºåˆ†å¿…éœ€å’Œå¯é€‰æ•°æ®æº
                    is_data_empty = df is None or (isinstance(df, pd.DataFrame) and df.empty) or (isinstance(df, dict) and not df)
                    
                    # å®šä¹‰å¯é€‰æ•°æ®æºï¼ˆè¿™äº›æ•°æ®æºä¸ºç©ºæ—¶ä¸åº”è¯¥å¯¼è‡´æ•´ä¸ªå¤„ç†å¤±è´¥ï¼‰
                    optional_sources = ['p2p_trade_raw', 'otc_trade_raw', 'margin_trade_raw', 'pay_trade_raw']
                    
                    if is_data_empty:
                        if source_id in optional_sources:
                            # å¯é€‰æ•°æ®æºä¸ºç©ºæ—¶ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†
                            print(f"        ğŸŸ¡ è­¦å‘Šï¼šå¯é€‰æ•°æ®æº '{source_id}' æ— æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
                            extracted_data[source_id] = pd.DataFrame()  # å­˜å‚¨ç©ºDataFrameï¼Œé¿å…åç»­å¤„ç†æŠ¥é”™
                        else:
                            # å¿…éœ€æ•°æ®æºä¸ºç©ºæ—¶ï¼Œè®°å½•ä¸ºå¤±è´¥
                            failed_sources.append({
                                'source_id': source_id,
                                'reason': f"CSVæ•°æ®æº '{source_id}' æœªæ‰¾åˆ°æ•°æ®æˆ–ä¸ºç©º",
                                'layout': layout
                            })
                            print(f"        âŒ æ•°æ®æº '{source_id}' æå–å¤±è´¥ï¼šæ— æ•°æ®")
                    else:
                        extracted_data[source_id] = df
                        success_sources.append(source_id)
                        
                        # --- ğŸ“Š ç®€åŒ–æ•°æ®æ¦‚è§ˆ ---
                        if isinstance(df, pd.DataFrame):
                            print(f"        âœ… æå–åˆ° {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—æ•°æ®")
                            if not df.empty:
                                # åªæ˜¾ç¤ºå‰3ä¸ªå­—æ®µçš„ç¤ºä¾‹æ•°æ®ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
                                sample_data = {}
                                for i, (k, v) in enumerate(df.iloc[0].items()):
                                    if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå­—æ®µ
                                        break
                                    sample_data[k] = str(v)[:20] + "..." if len(str(v)) > 20 else str(v)
                                print(f"        ğŸ“‹ è¡¨å¤´: {list(df.columns)[:5]}...")
                        elif isinstance(df, dict):
                            print(f"        âœ… æå–åˆ°å­—å…¸æ•°æ®: {len(df)} ä¸ªé”®")
                        else:
                            print(f"        âœ… æå–åˆ°æ•°æ®: {type(df)}")
                            
                except Exception as e:
                    failed_sources.append({
                        'source_id': source_id,
                        'reason': f"å¤„ç†CSVæ•°æ®æº '{source_id}' æ—¶å‡ºé”™: {str(e)}",
                        'layout': layout
                    })
                    print(f"        âŒ æ•°æ®æº '{source_id}' å¤„ç†å¼‚å¸¸: {str(e)}")
        
        except Exception as e:
            print(f"    âŒ è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            # å°†æ‰€æœ‰æºéƒ½æ ‡è®°ä¸ºå¤±è´¥
            for source in sources_config:
                failed_sources.append({
                    'source_id': source.get('source_id', 'unknown'),
                    'reason': f"æ— æ³•è¯»å–CSVæ–‡ä»¶: {str(e)}",
                    'layout': source.get('data_layout', 'unknown')
                })
            
    elif file_extension in ['.xls', '.xlsx']:
        # --- Excel æ–‡ä»¶å¤„ç†é€»è¾‘ (å¢å¼ºéªŒè¯) ---
        print(f"    ğŸ“Š è§£æExcelæ–‡ä»¶: {excel_path.name}")
        try:
            xls = pd.ExcelFile(excel_path)
            available_sheets = xls.sheet_names
            print(f"    ğŸ“‹ æ–‡ä»¶åŒ…å«å·¥ä½œè¡¨: {available_sheets}")
        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {excel_path}")
            # å°†æ‰€æœ‰æºéƒ½æ ‡è®°ä¸ºå¤±è´¥
            for source in sources_config:
                failed_sources.append({
                    'source_id': source.get('source_id', 'unknown'),
                    'reason': f"Excelæ–‡ä»¶æœªæ‰¾åˆ°: {excel_path}",
                    'layout': source.get('data_layout', 'unknown')
                })
            return None
        except Exception as e:
            print(f"    âŒ æ— æ³•æ‰“å¼€Excelæ–‡ä»¶: {e}")
            # å°†æ‰€æœ‰æºéƒ½æ ‡è®°ä¸ºå¤±è´¥
            for source in sources_config:
                failed_sources.append({
                    'source_id': source.get('source_id', 'unknown'),
                    'reason': f"æ— æ³•æ‰“å¼€Excelæ–‡ä»¶: {str(e)}",
                    'layout': source.get('data_layout', 'unknown')
                })
            return None

        for source in sources_config:
            source_id = source.get('source_id', 'unknown')
            
            # å®‰å…¨è·å–å·¥ä½œè¡¨å
            if 'worksheet_name' not in source:
                failed_sources.append({
                    'source_id': source_id,
                    'reason': f"æ¨¡æ¿é…ç½®é”™è¯¯ï¼šæ•°æ®æº '{source_id}' ç¼ºå°‘ worksheet_name å­—æ®µ",
                    'layout': source.get('data_layout', 'unknown')
                })
                print(f"    âŒ æ¨¡æ¿é…ç½®é”™è¯¯ï¼šæ•°æ®æº '{source_id}' ç¼ºå°‘å·¥ä½œè¡¨åé…ç½®")
                continue
                
            sheet_name = source['worksheet_name']
            layout = source.get('data_layout', 'tabular')
            print(f"    ğŸ“„ è¯»å–å·¥ä½œè¡¨: '{sheet_name}' ({layout})")
            
            try:
                # é¦–å…ˆæ£€æŸ¥å·¥ä½œè¡¨æ˜¯å¦å­˜åœ¨
                if sheet_name not in available_sheets:
                    failed_sources.append({
                        'source_id': source_id,
                        'reason': f"å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨",
                        'sheet_name': sheet_name,
                        'available_sheets': available_sheets,
                        'layout': layout
                    })
                    print(f"        âŒ å·¥ä½œè¡¨ '{sheet_name}' ä¸å­˜åœ¨ï¼")
                    print(f"        ğŸ“‹ å¯ç”¨å·¥ä½œè¡¨: {available_sheets}")
                    continue
                
                # --- è°ƒåº¦ä¸­å¿ƒ ---
                if layout == 'tabular':
                    df = pd.read_excel(xls, sheet_name=sheet_name, header=source.get('header_row', 1) - 1, nrows=source.get('nrows', None))
                
                elif layout == 'merged_key_value':
                    raw_df = pd.read_excel(xls, sheet_name=sheet_name, header=source.get('header_row', 1) - 1)
                    df = _parse_merged_key_value(raw_df, source)
                    
                else: # å¤„ç† find_subtable_by_header å’Œ form_layout
                    full_sheet_df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
                    header_aliases = source.get('section_header_aliases', []) 
                    
                    if layout == 'find_subtable_by_header':
                        df = _parse_tabular_subtable(full_sheet_df, header_aliases, source['header_offset'])
                    elif layout == 'form_layout':
                        df = _parse_form_subtable(full_sheet_df, header_aliases, source['header_offset'])
                    else:
                        print(f"        ğŸŸ¡ è­¦å‘Š: æœªçŸ¥å¸ƒå±€ç±»å‹ '{layout}'ï¼Œè·³è¿‡")
                        df = pd.DataFrame()
                
                # âœ… æ™ºèƒ½éªŒè¯ï¼šåŒºåˆ†å¿…éœ€å’Œå¯é€‰æ•°æ®æº
                is_data_empty = df is None or (isinstance(df, pd.DataFrame) and df.empty) or (isinstance(df, dict) and not df)
                
                # å®šä¹‰å¯é€‰æ•°æ®æºï¼ˆè¿™äº›æ•°æ®æºä¸ºç©ºæ—¶ä¸åº”è¯¥å¯¼è‡´æ•´ä¸ªå¤„ç†å¤±è´¥ï¼‰
                optional_sources = ['p2p_trade_raw', 'otc_trade_raw', 'margin_trade_raw', 'pay_trade_raw']
                
                if is_data_empty:
                    if source_id in optional_sources:
                        # å¯é€‰æ•°æ®æºä¸ºç©ºæ—¶ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†
                        print(f"        ğŸŸ¡ è­¦å‘Šï¼šå¯é€‰æ•°æ®æº '{source_id}' æ— æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
                        extracted_data[source_id] = pd.DataFrame()  # å­˜å‚¨ç©ºDataFrameï¼Œé¿å…åç»­å¤„ç†æŠ¥é”™
                    else:
                        # å¿…éœ€æ•°æ®æºä¸ºç©ºæ—¶ï¼Œè®°å½•ä¸ºå¤±è´¥
                        failed_sources.append({
                            'source_id': source_id,
                            'reason': f"å·¥ä½œè¡¨ '{sheet_name}' ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®",
                            'sheet_name': sheet_name,
                            'layout': layout
                        })
                        print(f"        âŒ æ•°æ®æº '{source_id}' æå–å¤±è´¥ï¼šå·¥ä½œè¡¨ '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®")
                else:
                    extracted_data[source_id] = df
                    success_sources.append(source_id)
                    
                    # --- ğŸ“Š ç®€åŒ–æ•°æ®æ¦‚è§ˆ ---
                    if isinstance(df, pd.DataFrame):
                        print(f"        âœ… æå–åˆ° {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—æ•°æ®")
                        if not df.empty:
                            print(f"        ğŸ“‹ è¡¨å¤´: {list(df.columns)[:5]}...")
                    elif isinstance(df, dict):
                        print(f"        âœ… æå–åˆ°å­—å…¸æ•°æ®: {len(df)} ä¸ªé”®")
                    else:
                        print(f"        âœ… æå–åˆ°æ•°æ®: {type(df)}")
                        
            except Exception as e:
                failed_sources.append({
                    'source_id': source_id,
                    'reason': f"å¤„ç†å·¥ä½œè¡¨ '{sheet_name}' æ—¶å‡ºé”™: {str(e)}",
                    'sheet_name': sheet_name,
                    'layout': layout
                })
                print(f"        âŒ æ•°æ®æº '{source_id}' å¤„ç†å¼‚å¸¸: {str(e)}")
                
    else:
        print(f"    ğŸŸ¡ è­¦å‘Š: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ '{file_extension}'ï¼Œè·³è¿‡æ–‡ä»¶")
        for source in sources_config:
            failed_sources.append({
                'source_id': source.get('source_id', 'unknown'),
                'reason': f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}",
                'layout': source.get('data_layout', 'unknown')
            })
        return None
    
    # âœ… æ™ºèƒ½æœ€ç»ˆéªŒè¯ï¼šåªå¯¹æ ¸å¿ƒå¿…éœ€æ•°æ®æºè¿›è¡Œä¸¥æ ¼æ£€æŸ¥
    total_sources = len(sources_config)
    success_count = len(success_sources)
    failed_count = len(failed_sources)
    
    # å®šä¹‰å¯é€‰æ•°æ®æº
    optional_sources = ['p2p_trade_raw', 'otc_trade_raw', 'margin_trade_raw', 'pay_trade_raw']
    
    # è¿‡æ»¤å‡ºçœŸæ­£å…³é”®çš„å¤±è´¥ï¼ˆæ’é™¤å¯é€‰æ•°æ®æºçš„å¤±è´¥ï¼‰
    critical_failures = [f for f in failed_sources if f['source_id'] not in optional_sources]
    
    print(f"    ğŸ“Š æ•°æ®æºæå–ç»“æœ: æˆåŠŸ {success_count}/{total_sources}")
    
    if failed_sources:
        print(f"    âŒ å¤±è´¥çš„æ•°æ®æº ({failed_count}):")
        for failed in failed_sources:
            source_id = failed['source_id']
            is_optional = source_id in optional_sources
            status_icon = "ğŸŸ¡" if is_optional else "âŒ"
            status_text = "(å¯é€‰)" if is_optional else "(å¿…éœ€)"
            print(f"        {status_icon} {source_id} {status_text}: {failed['reason']}")
    
    # åªæœ‰å½“æ ¸å¿ƒå¿…éœ€æ•°æ®æºå¤±è´¥æ—¶æ‰æŠ›å‡ºé”™è¯¯
    if critical_failures:
        # æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸ï¼ŒåŒ…å«è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯
        from .error_handler import ETLError, ErrorType
        
        failed_details = []
        missing_sheets = []
        missing_worksheets_info = []
        
        for failed in critical_failures:
            if 'sheet_name' in failed:
                missing_sheets.append(failed['sheet_name'])
                # æ”¶é›†ç¼ºå¤±å·¥ä½œè¡¨çš„è¯¦ç»†ä¿¡æ¯
                missing_worksheets_info.append({
                    'sheet_name': failed['sheet_name'],
                    'source_id': failed['source_id'],
                    'reason': failed['reason']
                })
            failed_details.append(f"â€¢ {failed['source_id']}: {failed['reason']}")
        
        if missing_sheets:
            # ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            missing_count = len(missing_sheets)
            if missing_count == 1:
                sheet_name = missing_sheets[0]
                error_message = f"æ‚¨çš„Excelæ–‡ä»¶ä¸­ç¼ºå°‘ã€{sheet_name}ã€‘å·¥ä½œè¡¨"
                details = f"ç³»ç»Ÿéœ€è¦è¯»å–åä¸ºã€{sheet_name}ã€‘çš„å·¥ä½œè¡¨ï¼Œä½†åœ¨æ‚¨ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¿™ä¸ªå·¥ä½œè¡¨ã€‚"
            else:
                # æ˜¾ç¤ºæ‰€æœ‰ç¼ºå¤±çš„å·¥ä½œè¡¨ï¼Œä¸åšæˆªæ–­
                sheet_list = 'ã€‘ã€ã€'.join(missing_sheets)
                error_message = f"æ‚¨çš„Excelæ–‡ä»¶ä¸­ç¼ºå°‘{missing_count}ä¸ªå·¥ä½œè¡¨ï¼šã€{sheet_list}ã€‘"
                details = f"ç³»ç»Ÿéœ€è¦è¯»å–ä»¥ä¸‹å·¥ä½œè¡¨ï¼šã€{sheet_list}ã€‘ï¼Œä½†åœ¨æ‚¨ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¿™äº›å·¥ä½œè¡¨ã€‚"
            
            # ç”Ÿæˆè¯¦ç»†çš„å»ºè®®
            suggestions = [
                f"ğŸ” è¯·æ£€æŸ¥æ‚¨çš„Excelæ–‡ä»¶æ˜¯å¦åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨ï¼š",
                f"   ç¼ºå°‘çš„å·¥ä½œè¡¨ï¼šã€{', '.join(missing_sheets)}ã€‘",
                "",
                f"ğŸ“‹ æ‚¨çš„æ–‡ä»¶å½“å‰åŒ…å«çš„å·¥ä½œè¡¨ï¼š",
                f"   {', '.join(available_sheets)}",
                "",
                "ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š",
                "   â€¢ ç¡®è®¤å·¥ä½œè¡¨åç§°æ‹¼å†™æ˜¯å¦æ­£ç¡®ï¼ˆæ³¨æ„åŒºåˆ†å¤§å°å†™ï¼‰",
                "   â€¢ æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„äº¤æ˜“å¹³å°",
                "   â€¢ é‡æ–°ä»äº¤æ˜“å¹³å°å¯¼å‡ºå®Œæ•´çš„æ•°æ®æ–‡ä»¶",
                "   â€¢ ç¡®è®¤æ‚¨é€‰æ‹©çš„å¹³å°ä¸æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ¹é…"
            ]
        else:
            error_message = "æ–‡ä»¶æ•°æ®æå–å¤±è´¥"
            details = "ç³»ç»Ÿæ— æ³•ä»æ‚¨ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–åˆ°å¿…éœ€çš„æ•°æ®ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–æ–‡ä»¶å†…å®¹ä¸å®Œæ•´ã€‚"
            suggestions = [
                "ğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š",
                "   â€¢ æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦ä¸ºExcel(.xlsx/.xls)æ ¼å¼",
                "   â€¢ ç¡®è®¤æ–‡ä»¶å†…å®¹å®Œæ•´ï¼Œæ²¡æœ‰æŸå",
                "   â€¢ é‡æ–°ä»äº¤æ˜“å¹³å°å¯¼å‡ºæ•°æ®æ–‡ä»¶",
                "   â€¢ ç¡®è®¤é€‰æ‹©çš„äº¤æ˜“å¹³å°ä¸æ–‡ä»¶å†…å®¹åŒ¹é…"
            ]
        
        raise ETLError(
            error_type=ErrorType.DATA_VALIDATION_ERROR,
            message=error_message,
            details=details,
            suggestions=suggestions
        )
    else:
        # å³ä½¿æœ‰å¯é€‰æ•°æ®æºå¤±è´¥ï¼Œåªè¦æ ¸å¿ƒæ•°æ®æºæˆåŠŸå°±ç»§ç»­å¤„ç†
        if failed_sources:
            print("    ğŸŸ¡ æ³¨æ„ï¼šéƒ¨åˆ†å¯é€‰æ•°æ®æºæ— æ•°æ®ï¼Œä½†æ ¸å¿ƒæ•°æ®æºå®Œæ•´ï¼Œç»§ç»­å¤„ç†...")
        else:
            print("    ğŸ¯ æ‰€æœ‰æ•°æ®æºæå–æˆåŠŸï¼")
    return extracted_data

def process_single_destination(destination_config: dict, all_extracted_data: dict, function_registry: dict, source_file_name: str) -> pd.DataFrame:
    """
    (V5ç‰ˆ - æ™ºèƒ½å¤„ç†ç‰ˆ)
    èƒ½å¤Ÿæ™ºèƒ½åœ°åˆ¤æ–­ä¸»æ•°æ®æºæ˜¯å¤šè¡Œè¡¨æ ¼(DataFrame)è¿˜æ˜¯å•æ¡è®°å½•(dict)ã€‚
    """
    primary_source_name = destination_config.get('primary_source')
    primary_data = all_extracted_data.get(primary_source_name)

    if primary_data is None:
        print(f"  - âŒ é”™è¯¯: æœªæ‰¾åˆ°ä¸»æ•°æ®æº '{primary_source_name}'ã€‚")
        return None
    
    output_rows = []
    
    # --- æ ¸å¿ƒå‡çº§ï¼šåˆ¤æ–­åŸææ–™æ˜¯"å¤šè¡Œè¡¨æ ¼"è¿˜æ˜¯"å•æ¡è®°å½•" ---
    if isinstance(primary_data, pd.DataFrame):
        # å¦‚æœæ˜¯å¤šè¡Œè¡¨æ ¼ï¼Œæˆ‘ä»¬åƒä»¥å‰ä¸€æ ·ï¼Œé€è¡Œå¤„ç†
        for index, source_row in primary_data.iterrows():
            new_row = _process_one_row(destination_config, source_row, primary_data.columns, all_extracted_data, function_registry, source_file_name)
            output_rows.append(new_row)
    elif isinstance(primary_data, dict):
        # å¦‚æœæ˜¯å•æ¡è®°å½•ï¼ˆå­—å…¸ï¼‰ï¼Œæˆ‘ä»¬åªå¤„ç†ä¸€æ¬¡ï¼Œä¸éœ€è¦å¾ªç¯ï¼
        # æˆ‘ä»¬æŠŠè¿™ä¸ªå­—å…¸åŒ…è£…æˆpandasçš„Seriesï¼Œè®©ä¸‹æ¸¸å‡½æ•°å¯ä»¥ç»Ÿä¸€å¤„ç†
        source_row = pd.Series(primary_data)
        new_row = _process_one_row(destination_config, source_row, primary_data.keys(), all_extracted_data, function_registry, source_file_name)
        output_rows.append(new_row)
    
    return pd.DataFrame(output_rows)

def _process_one_row(destination_config, source_row, source_columns, all_extracted_data, function_registry, source_file_name):
    """
    (æœ€ç»ˆç‰ˆ) è´Ÿè´£å¤„ç†å•è¡Œæ•°æ®çš„å®Œæ•´æ˜ å°„é€»è¾‘ã€‚
    """
    mappings = destination_config.get('mappings', [])
    standard_fields = {'source_file_name': source_file_name}
    
    extra_fields = source_row.get('extra_data', {})
    if not isinstance(extra_fields, dict):
        extra_fields = {}

    for rule in mappings:
        target_field = rule['target_field']
        raw_value = None
        
        if 'static_value' in rule:
            raw_value = rule['static_value']
        elif 'lookup_source' in rule:
            lookup_data = all_extracted_data.get(rule['lookup_source'])
            is_lookup_data_valid = False
            if isinstance(lookup_data, dict) and lookup_data:
                is_lookup_data_valid = True
            elif isinstance(lookup_data, pd.DataFrame) and not lookup_data.empty:
                is_lookup_data_valid = True
            if is_lookup_data_valid:
                lookup_row = pd.Series(lookup_data) if isinstance(lookup_data, dict) else lookup_data.iloc[0]
                field_name = find_field_from_aliases(lookup_row.index, rule.get('source_field_aliases', []))
                if field_name:
                    raw_value = lookup_row.get(field_name)
        else:
            field_name = find_field_from_aliases(source_columns, rule.get('source_field_aliases', []))
            if field_name:
                raw_value = source_row.get(field_name)

        transformed_value = raw_value
        for trans in rule.get('transformations', []):
            func = function_registry.get(trans['function'])
            if func:
                transformed_value = func(transformed_value)

        if rule.get('in_extra', False):
            extra_fields[target_field] = transformed_value
        else:
            standard_fields[target_field] = transformed_value
    
    if extra_fields:
        def _json_converter(o):
            if pd.isna(o):
                return None
            if isinstance(o, pd.Timestamp):
                return o.isoformat()
            if isinstance(o, Decimal):
                return str(o)
            if hasattr(o, 'item'):
                return o.item()
            raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")
        
        sanitized_extra_fields = {k: _sanitize_for_json(v) for k, v in extra_fields.items()}
        standard_fields['extra_data'] = json.dumps(sanitized_extra_fields, default=_json_converter, ensure_ascii=False)
    
    return standard_fields

def _sanitize_for_json(obj):
    """
    (å…¨æ–°çš„"æ·±åº¦æ¸…æ´"å·¥å…·)
    é€’å½’åœ°éå†ä¸€ä¸ªå­—å…¸æˆ–åˆ—è¡¨ï¼Œå°†æ‰€æœ‰Pandasçš„ç©ºå€¼(nan, NaTç­‰)éƒ½æ›¿æ¢æˆNoneã€‚
    """
    if isinstance(obj, dict):
        return {k: _sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_sanitize_for_json(elem) for elem in obj]
    elif pd.isna(obj):
        return None
    return obj
