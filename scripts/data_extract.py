# data_extract.py - æ•°æ®å¤„ç†è½¦é—´ (V2 - è¡¨å•å¤„ç†å‡çº§ç‰ˆ)

from pathlib import Path
import pandas as pd
import json
from decimal import Decimal

# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
def find_field_from_aliases(columns, aliases):
    for alias in aliases:
        if alias in columns:
            return alias
    return None

# --- é«˜çº§è§£æå·¥å…· (æ–°å¢ä¸€ä¸ª + ä¼˜åŒ–ä¸€ä¸ª) ---

def _find_subtable_start_row(sheet_df: pd.DataFrame, section_header: str, header_offset: int) -> int:
    """(å†…éƒ¨å·¥å…·) åœ¨å·¥ä½œè¡¨ä¸­ï¼Œæ ¹æ®åˆ†å—æ ‡é¢˜ï¼Œåªè¿”å›å­è¡¨çš„è¡¨å¤´è¡Œå·ã€‚"""
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ ‡é¢˜çš„å•å…ƒæ ¼
    # a.stack()å°†DataFrameå˜æˆä¸€ä¸ªSeries, .str.contains()è¿›è¡ŒæŸ¥æ‰¾, .dropna()å»æ‰ä¸åŒ¹é…çš„
    matches = sheet_df.stack().str.contains(section_header, na=False).dropna()
    if matches.any():
        # matches.index[0][0]å¯ä»¥è·å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„è¡Œå·
        header_row_index = matches.index[0][0]
        return header_row_index + header_offset
    
    print(f"  - ğŸŸ¡ è­¦å‘Š: åœ¨å·¥ä½œè¡¨ä¸­æœªæ‰¾åˆ°æ ‡é¢˜ä¸º '{section_header}' çš„å­è¡¨ã€‚")
    return None

def _parse_tabular_subtable(sheet_df: pd.DataFrame, section_header: str, header_offset: int) -> pd.DataFrame:
    """(æ—§å·¥å…·çš„ä¼˜åŒ–ç‰ˆ) è§£æä¸€ä¸ªæ ‡å‡†çš„å¤šè¡Œå­è¡¨ã€‚"""
    header_row = _find_subtable_start_row(sheet_df, section_header, header_offset)
    if header_row is None: return pd.DataFrame()
    
    sub_table = sheet_df.iloc[header_row+1:].copy()
    sub_table.columns = sheet_df.iloc[header_row].values
    sub_table.reset_index(drop=True, inplace=True)
    sub_table.dropna(how='all', inplace=True)
    return sub_table

def _parse_form_subtable(sheet_df: pd.DataFrame, section_header: str, header_offset: int) -> dict:
    """(å…¨æ–°çš„â€œè¡¨å•å¤„ç†â€å·¥å…·) è§£æä¸€ä¸ªâ€œè¡¨å•å¼â€çš„å­è¡¨ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå•è¡Œçš„å­—å…¸ã€‚"""
    header_row = _find_subtable_start_row(sheet_df, section_header, header_offset)
    if header_row is None: return {}
    
    # åœ¨è¡¨å•ä¸­ï¼Œæˆ‘ä»¬å‡è®¾è¡¨å¤´åœ¨ header_rowï¼Œè€Œæ•°æ®å°±åœ¨å®ƒçš„ä¸‹ä¸€è¡Œ
    data_row_index = header_row + 1
    
    # å°†è¡¨å¤´è¡Œå’Œæ•°æ®è¡Œï¼Œæ‰“åŒ…æˆä¸€ä¸ªå­—å…¸ {è¡¨å¤´: æ•°æ®}
    form_data = pd.Series(
        sheet_df.iloc[data_row_index].values,
        index=sheet_df.iloc[header_row].values
    ).to_dict()
    
    return form_data

# --- æ ¸å¿ƒâ€œå–è´§å‘˜â€å‡½æ•° (å‡çº§ç‰ˆ) ---

def extract_data_from_sources(excel_path: Path, sources_config: list) -> dict:
    """
    (V4ç‰ˆ - æ™ºèƒ½è°ƒåº¦ç‰ˆ)
    èƒ½å¤Ÿæ ¹æ®'data_layout'è°ƒç”¨ä¸åŒçš„ã€ä¸“é—¨çš„è§£æå™¨ã€‚
    """
    print(f"\n--- æ­£åœ¨ä»Excelæå–æ•°æ®å— ---")
    extracted_data = {}
    try:
        xls = pd.ExcelFile(excel_path)
    except FileNotFoundError:
        print(f"âŒ Excelæ–‡ä»¶æœªæ‰¾åˆ°: {excel_path}")
        return None

    for source in sources_config:
        sheet_name = source['worksheet_name']
        layout = source.get('data_layout', 'tabular')
        print(f"  - æ­£åœ¨è¯»å–å·¥ä½œè¡¨: '{sheet_name}' (ID: '{source['source_id']}', å¸ƒå±€: {layout})")
        
        if layout == 'tabular':
            df = pd.read_excel(xls, sheet_name=sheet_name, header=source.get('header_row', 1) - 1, nrows=source.get('nrows', None))
            extracted_data[source['source_id']] = df
        else:
            full_sheet_df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            if layout == 'find_subtable_by_header':
                data = _parse_tabular_subtable(full_sheet_df, source['section_header'], source['header_offset'])
            elif layout == 'form_layout':
                data = _parse_form_subtable(full_sheet_df, source['section_header'], source['header_offset'])
            else:
                print(f"  - ğŸŸ¡ è­¦å‘Š: æœªçŸ¥çš„ data_layout ç±»å‹ '{layout}'ï¼Œè·³è¿‡ã€‚")
                data = None
            
            extracted_data[source['source_id']] = data
            
    print("âœ… æ‰€æœ‰æ•°æ®å—æå–å®Œæˆã€‚")
    return extracted_data

# --- â€œé¦–å¸­å·¥åŒ â€å‡½æ•° (å‡çº§ç‰ˆ) ---

def process_single_destination(destination_config: dict, all_extracted_data: dict, function_registry: dict, source_file_name: str) -> pd.DataFrame:
    """
    (V5ç‰ˆ - æ™ºèƒ½å¤„ç†ç‰ˆ)
    èƒ½å¤Ÿæ™ºèƒ½åœ°åˆ¤æ–­ä¸»æ•°æ®æºæ˜¯å¤šè¡Œè¡¨æ ¼(DataFrame)è¿˜æ˜¯å•æ¡è®°å½•(dict)ã€‚
    """
    primary_source_name = destination_config.get('primary_source')
    primary_data = all_extracted_data.get(primary_source_name)

    if primary_data is None:
        print(f"  - âŒ é”™è¯¯: æœªæ‰¾åˆ°ä¸»æ•°æ®æº '{primary_source_name}'ã€‚")
        return None
    
    output_rows = []
    
    # --- æ ¸å¿ƒå‡çº§ï¼šåˆ¤æ–­åŸææ–™æ˜¯â€œå¤šè¡Œè¡¨æ ¼â€è¿˜æ˜¯â€œå•æ¡è®°å½•â€ ---
    if isinstance(primary_data, pd.DataFrame):
        # å¦‚æœæ˜¯å¤šè¡Œè¡¨æ ¼ï¼Œæˆ‘ä»¬åƒä»¥å‰ä¸€æ ·ï¼Œé€è¡Œå¤„ç†
        for index, source_row in primary_data.iterrows():
            new_row = _process_one_row(destination_config, source_row, primary_data.columns, all_extracted_data, function_registry, source_file_name)
            output_rows.append(new_row)
    elif isinstance(primary_data, dict):
        # å¦‚æœæ˜¯å•æ¡è®°å½•ï¼ˆå­—å…¸ï¼‰ï¼Œæˆ‘ä»¬åªå¤„ç†ä¸€æ¬¡ï¼Œä¸éœ€è¦å¾ªç¯ï¼
        # æˆ‘ä»¬æŠŠè¿™ä¸ªå­—å…¸åŒ…è£…æˆpandasçš„Seriesï¼Œè®©ä¸‹æ¸¸å‡½æ•°å¯ä»¥ç»Ÿä¸€å¤„ç†
        source_row = pd.Series(primary_data)
        new_row = _process_one_row(destination_config, source_row, primary_data.keys(), all_extracted_data, function_registry, source_file_name)
        output_rows.append(new_row)
    
    return pd.DataFrame(output_rows)

def _process_one_row(destination_config, source_row, source_columns, all_extracted_data, function_registry, source_file_name):
    """
    (è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„å†…éƒ¨å·¥å…·) ä¸“é—¨è´Ÿè´£å¤„ç†å•è¡Œæ•°æ®çš„å®Œæ•´æ˜ å°„é€»è¾‘ã€‚
    """
    mappings = destination_config.get('mappings', [])
    standard_fields = {'source_file_name': source_file_name}
    extra_fields = {}

    for rule in mappings:
        target_field = rule['target_field']
        raw_value = None
        if 'static_value' in rule:
            raw_value = rule['static_value']
        elif 'lookup_source' in rule:
            lookup_data = all_extracted_data.get(rule['lookup_source'])
            if lookup_data is not None:
                # å‡è®¾lookupæ€»æ˜¯å•æ¡è®°å½•(dict)æˆ–å•è¡Œè¡¨(DataFrame)
                lookup_row = pd.Series(lookup_data) if isinstance(lookup_data, dict) else lookup_data.iloc[0]
                field_name = find_field_from_aliases(lookup_row.index, rule.get('source_field_aliases', []))
                if field_name:
                    raw_value = lookup_row.get(field_name)
        else:
            field_name = find_field_from_aliases(source_columns, rule.get('source_field_aliases', []))
            if field_name:
                raw_value = source_row.get(field_name)

        transformed_value = raw_value
        for trans in rule.get('transformations', []):
            func = function_registry.get(trans['function'])
            if func: transformed_value = func(transformed_value)

        if rule.get('in_extra', False):
            extra_fields[target_field] = transformed_value
        else:
            standard_fields[target_field] = transformed_value
    
    if extra_fields:
        def json_converter(o):
            if isinstance(o, (pd.Timestamp, pd.NaT.__class__)): return o.isoformat() if pd.notna(o) else None
            if isinstance(o, Decimal): return str(o)
            if hasattr(o, 'item'): return o.item()
            raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")
        standard_fields['extra_data'] = json.dumps(extra_fields, default=json_converter, ensure_ascii=False)
    
    return standard_fields