# data_extract.py - æ•°æ®å¤„ç†è½¦é—´ (V8 - å®Œæ•´æ”¯æŒCSVçš„4ç§è¯†åˆ«æ–¹æ³•)

from pathlib import Path
import pandas as pd
import json
from decimal import Decimal

# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
def find_field_from_aliases(columns, aliases):
    for alias in aliases:
        if alias in columns:
            return alias
    return None

# --- é«˜çº§è§£æå·¥å…· (æ–°å¢ä¸€ä¸ª + ä¼˜åŒ–ä¸€ä¸ª) ---

def _find_subtable_start_row(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> int:
    """
    (V3 - "å¤šè·¯æ ‡"ç‰ˆ) 
    åœ¨å·¥ä½œè¡¨ä¸­ï¼Œæ ¹æ®ä¸€ä¸ª"è·¯æ ‡"åˆ«ååˆ—è¡¨ï¼Œä¾æ¬¡å°è¯•å¯»æ‰¾ï¼Œå¹¶è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„å­è¡¨çš„è¡¨å¤´è¡Œå·ã€‚
    """
    # --- æ ¸å¿ƒå‡çº§ï¼šéå†"è·¯æ ‡"åˆ—è¡¨ ---
    for header in section_header_aliases:
        print(f"  - å°è¯•å®šä½è·¯æ ‡: '{header}'...")
        matches = sheet_df.stack() == header
        true_matches = matches[matches].index

        if len(true_matches) > 0:
            # åªè¦æ‰¾åˆ°äº†ç¬¬ä¸€ä¸ªï¼Œå°±ç«‹åˆ»è¿”å›ç»“æœï¼Œä¸å†ç»§ç»­å¯»æ‰¾ï¼
            header_row_index = true_matches[0][0]
            print(f"    âœ… æˆåŠŸï¼åœ¨ç¬¬ {header_row_index} è¡Œæ‰¾åˆ°äº†ç²¾ç¡®åŒ¹é…çš„è·¯æ ‡ã€‚")
            return header_row_index + header_offset
    
    # å¦‚æœæŠŠåˆ—è¡¨é‡Œæ‰€æœ‰è·¯æ ‡éƒ½è¯•å®Œäº†ï¼Œè¿˜æ˜¯æ²¡æ‰¾åˆ°
    print(f"  - ğŸŸ¡ è­¦å‘Š: åœ¨å·¥ä½œè¡¨ä¸­æœªèƒ½æ‰¾åˆ°ä»»ä½•ä¸€ä¸ªæŒ‡å®šçš„è·¯æ ‡: {section_header_aliases}ã€‚")
    return None

def _parse_tabular_subtable(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> pd.DataFrame:
    """
    (V2 - æ™ºèƒ½ç»“æŸç‰ˆ) 
    è§£æä¸€ä¸ªæ ‡å‡†çš„å¤šè¡Œå­è¡¨ã€‚èƒ½å¤Ÿé€šè¿‡å¯»æ‰¾å…¨ç©ºè¡Œæ¥æ™ºèƒ½åˆ¤æ–­å­è¡¨çš„ç»“æŸä½ç½®ã€‚
    """
    header_row = _find_subtable_start_row(sheet_df, section_header_aliases, header_offset)
    if header_row is None:
        return pd.DataFrame()
    
    # --- æ ¸å¿ƒå‡çº§ï¼šæ™ºèƒ½åˆ¤æ–­ç»“æŸä½ç½® ---
    potential_data = sheet_df.iloc[header_row + 1:]
    
    try:
        first_empty_row_index = potential_data.isnull().all(axis=1).eq(True).idxmax()
        if potential_data.loc[first_empty_row_index].isnull().all():
             end_index = first_empty_row_index
        else:
             end_index = None
    except ValueError:
        end_index = None

    if end_index is not None:
        actual_data = potential_data.loc[:end_index-1]
    else:
        actual_data = potential_data
        
    sub_table = actual_data.copy()
    sub_table.columns = sheet_df.iloc[header_row].values
    sub_table.reset_index(drop=True, inplace=True)
    sub_table.dropna(how='all', inplace=True)
    
    return sub_table

def _parse_form_subtable(sheet_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> dict:
    header_row = _find_subtable_start_row(sheet_df, section_header_aliases, header_offset)
    if header_row is None: 
        return {}
    
    data_row_index = header_row + 1
    
    form_data = pd.Series(
        sheet_df.iloc[data_row_index].values,
        index=sheet_df.iloc[header_row].values
    ).to_dict()
    
    return form_data

def _parse_merged_key_value(sheet_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    merged_cols = config['merged_columns']
    key_col = config['key_column']
    value_col = config['value_column']
    
    sheet_df[merged_cols] = sheet_df[merged_cols].fillna(method='ffill')
    sheet_df.dropna(subset=[merged_cols[0]], inplace=True)
    
    output_records = []
    for group_keys, group_df in sheet_df.groupby(merged_cols):
        record = dict(zip(merged_cols, group_keys))
        extra_data = group_df[[key_col, value_col]].dropna(subset=[key_col]).set_index(key_col).to_dict()[value_col]
        
        print(f"\n--- [è¯Šæ–­æ¢é’ˆ#1A] è§£æå™¨ä¸ºè®¾å¤‡ '{record.get('Device Name è®¾å¤‡åç§°')}' ç”Ÿæˆçš„ extra_data ---")
        print(extra_data)
        
        record['extra_data'] = extra_data
        output_records.append(record)
        
    final_df = pd.DataFrame(output_records)
    
    print(f"\n--- [è¯Šæ–­æ¢é’ˆ#1B] è§£æå™¨æœ€ç»ˆç”Ÿæˆçš„DataFrameé¢„è§ˆ (åº”åŒ…å«extra_dataåˆ—) ---")
    print(final_df.head().to_string())
    
    return final_df

# --- CSVä¸“ç”¨è§£æå‡½æ•° ---

def _parse_form_subtable_csv(csv_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> dict:
    """
    CSVç‰ˆæœ¬çš„è¡¨å•è§£æå‡½æ•°
    """
    header_row = _find_subtable_start_row(csv_df, section_header_aliases, header_offset)
    if header_row is None: 
        return {}
    
    data_row_index = header_row + 1
    if data_row_index >= len(csv_df):
        return {}
    
    form_data = pd.Series(
        csv_df.iloc[data_row_index].values,
        index=csv_df.iloc[header_row].values
    ).to_dict()
    
    return form_data

def _parse_tabular_subtable_csv(csv_df: pd.DataFrame, section_header_aliases: list, header_offset: int) -> pd.DataFrame:
    """
    CSVç‰ˆæœ¬çš„åŠ¨æ€å­è¡¨è§£æå‡½æ•°
    """
    header_row = _find_subtable_start_row(csv_df, section_header_aliases, header_offset)
    if header_row is None:
        return pd.DataFrame()
    
    potential_data = csv_df.iloc[header_row + 1:]
    
    try:
        first_empty_row_index = potential_data.isnull().all(axis=1).eq(True).idxmax()
        if potential_data.loc[first_empty_row_index].isnull().all():
             end_index = first_empty_row_index
        else:
             end_index = None
    except ValueError:
        end_index = None

    if end_index is not None:
        actual_data = potential_data.loc[:end_index-1]
    else:
        actual_data = potential_data
        
    sub_table = actual_data.copy()
    sub_table.columns = csv_df.iloc[header_row].values
    sub_table.reset_index(drop=True, inplace=True)
    sub_table.dropna(how='all', inplace=True)
    
    return sub_table

def _parse_merged_key_value_csv(csv_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    CSVç‰ˆæœ¬çš„å¤æ‚ä¸‰ç»´è¡¨è§£æå‡½æ•°
    """
    merged_cols = config['merged_columns']
    key_col = config['key_column']
    value_col = config['value_column']
    
    # å…ˆè®¾ç½®æ­£ç¡®çš„åˆ—å
    if 'header_row' in config:
        header_row = config['header_row'] - 1
        csv_df.columns = csv_df.iloc[header_row].values
        csv_df = csv_df.iloc[header_row + 1:].reset_index(drop=True)
    
    csv_df[merged_cols] = csv_df[merged_cols].fillna(method='ffill')
    csv_df.dropna(subset=[merged_cols[0]], inplace=True)
    
    output_records = []
    for group_keys, group_df in csv_df.groupby(merged_cols):
        record = dict(zip(merged_cols, group_keys))
        extra_data = group_df[[key_col, value_col]].dropna(subset=[key_col]).set_index(key_col).to_dict()[value_col]
        
        print(f"\n--- [CSVè¯Šæ–­æ¢é’ˆ#1A] è§£æå™¨ä¸ºè®¾å¤‡ '{record.get('Device Name è®¾å¤‡åç§°')}' ç”Ÿæˆçš„ extra_data ---")
        print(extra_data)
        
        record['extra_data'] = extra_data
        output_records.append(record)
        
    final_df = pd.DataFrame(output_records)
    
    print(f"\n--- [CSVè¯Šæ–­æ¢é’ˆ#1B] è§£æå™¨æœ€ç»ˆç”Ÿæˆçš„DataFrameé¢„è§ˆ (åº”åŒ…å«extra_dataåˆ—) ---")
    print(final_df.head().to_string())
    
    return final_df

# --- æ ¸å¿ƒ"å–è´§å‘˜"å‡½æ•° (å‡çº§ç‰ˆ) ---
def extract_data_from_sources(excel_path: Path, sources_config: list) -> dict:
    """
    (V8ç‰ˆ - å®Œæ•´æ”¯æŒCSVçš„4ç§è¯†åˆ«æ–¹æ³•)
    èƒ½å¤Ÿæ ¹æ®æ–‡ä»¶åç¼€åï¼Œæ™ºèƒ½é€‰æ‹©Excelæˆ–CSVçš„è§£æç­–ç•¥ã€‚
    """
    print(f"\n--- æ­£åœ¨ä» {excel_path.name} æå–æ•°æ®å— ---")
    extracted_data = {}
    file_extension = excel_path.suffix.lower()

    # --- âœ¨ æ ¸å¿ƒæ”¹é€ ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„è¯»å–ç­–ç•¥ âœ¨ ---
    if file_extension == '.csv':
        # --- CSV æ–‡ä»¶å¤„ç†é€»è¾‘ (æ”¯æŒ4ç§è¯†åˆ«æ–¹æ³•) ---
        print(f"  - æ£€æµ‹åˆ°CSVæ–‡ä»¶ï¼Œæ­£åœ¨è§£æ...")
        try:
            # é¦–å…ˆè¯»å–æ•´ä¸ªCSVæ–‡ä»¶ä½œä¸ºåŸå§‹æ•°æ®
            csv_df = pd.read_csv(excel_path, header=None)  # ä¸è®¾ç½®headerï¼Œä¿æŒåŸå§‹ç»“æ„
            
            # éå†é…ç½®ä¸­çš„æ¯ä¸ªæ•°æ®æºï¼Œæ ¹æ®layoutç±»å‹è¿›è¡Œå¤„ç†
            for source in sources_config:
                source_id = source['source_id']
                layout = source.get('data_layout', 'tabular')
                print(f"  - æ­£åœ¨å¤„ç†CSVæ•°æ®æº: '{source_id}' (å¸ƒå±€ç±»å‹: {layout})")
                
                # --- CSVè°ƒåº¦ä¸­å¿ƒï¼šæ ¹æ®å¸ƒå±€ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³• ---
                if layout == 'tabular':
                    # æ ‡å‡†è¡¨æ ¼ï¼šä½¿ç”¨æŒ‡å®šçš„header_row
                    header_row = source.get('header_row', 1) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
                    df = csv_df.copy()
                    if header_row < len(csv_df):
                        df.columns = csv_df.iloc[header_row].values  # è®¾ç½®è¡¨å¤´
                        df = df.iloc[header_row + 1:].reset_index(drop=True)  # å»æ‰è¡¨å¤´è¡Œ
                        df.dropna(how='all', inplace=True)  # æ¸…ç†ç©ºè¡Œ
                    else:
                        df = pd.DataFrame()
                    
                elif layout == 'form_layout':
                    # è¡¨å•å¸ƒå±€ï¼šå¯»æ‰¾ç‰¹å®šæ ‡é¢˜å¹¶è§£æé”®å€¼å¯¹
                    header_aliases = source.get('section_header_aliases', [])
                    header_offset = source.get('header_offset', 1)
                    df = _parse_form_subtable_csv(csv_df, header_aliases, header_offset)
                    
                elif layout == 'find_subtable_by_header':
                    # åŠ¨æ€å­è¡¨ï¼šå¯»æ‰¾ç‰¹å®šæ ‡é¢˜ä¸‹çš„è¡¨æ ¼æ•°æ®
                    header_aliases = source.get('section_header_aliases', [])
                    header_offset = source.get('header_offset', 1)
                    df = _parse_tabular_subtable_csv(csv_df, header_aliases, header_offset)
                    
                elif layout == 'merged_key_value':
                    # å¤æ‚ä¸‰ç»´è¡¨ï¼šå¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œé”®å€¼å¯¹
                    df = _parse_merged_key_value_csv(csv_df, source)
                    
                else:
                    print(f"  - ğŸŸ¡ è­¦å‘Š: æœªçŸ¥çš„CSVå¸ƒå±€ç±»å‹ '{layout}'ï¼Œä½¿ç”¨é»˜è®¤tabularå¤„ç†ã€‚")
                    # é»˜è®¤æŒ‰æ ‡å‡†è¡¨æ ¼å¤„ç†
                    header_row = source.get('header_row', 1) - 1
                    df = csv_df.copy()
                    if header_row < len(csv_df):
                        df.columns = csv_df.iloc[header_row].values
                        df = df.iloc[header_row + 1:].reset_index(drop=True)
                        df.dropna(how='all', inplace=True)
                    else:
                        df = pd.DataFrame()
                
                extracted_data[source_id] = df
                print(f"    - âœ… CSVæ•°æ®æº '{source_id}' å¤„ç†å®Œæˆï¼Œè·å¾— {len(df)} è¡Œæ•°æ®")
        
        except Exception as e:
            print(f"  - âŒ è¯»å–CSVæ–‡ä»¶ '{excel_path.name}' æ—¶å‡ºé”™: {e}")
            return None
            
    elif file_extension in ['.xls', '.xlsx']:
        # --- Excel æ–‡ä»¶å¤„ç†é€»è¾‘ (ä¿æŒä¸å˜) ---
        try:
            xls = pd.ExcelFile(excel_path)
        except FileNotFoundError:
            print(f"  - âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {excel_path}")
            return None

        for source in sources_config:
            sheet_name = source['worksheet_name']
            layout = source.get('data_layout', 'tabular')
            print(f"  - æ­£åœ¨è¯»å–å·¥ä½œè¡¨: '{sheet_name}' (ID: '{source['source_id']}', å¸ƒå±€: {layout})")
            
            # --- è°ƒåº¦ä¸­å¿ƒ ---
            if layout == 'tabular':
                df = pd.read_excel(xls, sheet_name=sheet_name, header=source.get('header_row', 1) - 1, nrows=source.get('nrows', None))
            
            elif layout == 'merged_key_value':
                raw_df = pd.read_excel(xls, sheet_name=sheet_name, header=source.get('header_row', 1) - 1)
                df = _parse_merged_key_value(raw_df, source)
                
            else: # å¤„ç† find_subtable_by_header å’Œ form_layout
                full_sheet_df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
                header_aliases = source.get('section_header_aliases', []) 
                
                if layout == 'find_subtable_by_header':
                    df = _parse_tabular_subtable(full_sheet_df, header_aliases, source['header_offset'])
                elif layout == 'form_layout':
                    df = _parse_form_subtable(full_sheet_df, header_aliases, source['header_offset'])
                else:
                    print(f"  - ğŸŸ¡ è­¦å‘Š: æœªçŸ¥çš„ data_layout ç±»å‹ '{layout}'ï¼Œè·³è¿‡ã€‚")
                    df = pd.DataFrame()
                
            extracted_data[source['source_id']] = df
    else:
        print(f"  - ğŸŸ¡ è­¦å‘Š: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ '{file_extension}'ï¼Œè·³è¿‡æ–‡ä»¶ã€‚")
        return None
            
    print("âœ… æ‰€æœ‰æ•°æ®å—æå–å®Œæˆã€‚")
    return extracted_data

def process_single_destination(destination_config: dict, all_extracted_data: dict, function_registry: dict, source_file_name: str) -> pd.DataFrame:
    """
    (V5ç‰ˆ - æ™ºèƒ½å¤„ç†ç‰ˆ)
    èƒ½å¤Ÿæ™ºèƒ½åœ°åˆ¤æ–­ä¸»æ•°æ®æºæ˜¯å¤šè¡Œè¡¨æ ¼(DataFrame)è¿˜æ˜¯å•æ¡è®°å½•(dict)ã€‚
    """
    primary_source_name = destination_config.get('primary_source')
    primary_data = all_extracted_data.get(primary_source_name)

    if primary_data is None:
        print(f"  - âŒ é”™è¯¯: æœªæ‰¾åˆ°ä¸»æ•°æ®æº '{primary_source_name}'ã€‚")
        return None
    
    output_rows = []
    
    # --- æ ¸å¿ƒå‡çº§ï¼šåˆ¤æ–­åŸææ–™æ˜¯"å¤šè¡Œè¡¨æ ¼"è¿˜æ˜¯"å•æ¡è®°å½•" ---
    if isinstance(primary_data, pd.DataFrame):
        # å¦‚æœæ˜¯å¤šè¡Œè¡¨æ ¼ï¼Œæˆ‘ä»¬åƒä»¥å‰ä¸€æ ·ï¼Œé€è¡Œå¤„ç†
        for index, source_row in primary_data.iterrows():
            new_row = _process_one_row(destination_config, source_row, primary_data.columns, all_extracted_data, function_registry, source_file_name)
            output_rows.append(new_row)
    elif isinstance(primary_data, dict):
        # å¦‚æœæ˜¯å•æ¡è®°å½•ï¼ˆå­—å…¸ï¼‰ï¼Œæˆ‘ä»¬åªå¤„ç†ä¸€æ¬¡ï¼Œä¸éœ€è¦å¾ªç¯ï¼
        # æˆ‘ä»¬æŠŠè¿™ä¸ªå­—å…¸åŒ…è£…æˆpandasçš„Seriesï¼Œè®©ä¸‹æ¸¸å‡½æ•°å¯ä»¥ç»Ÿä¸€å¤„ç†
        source_row = pd.Series(primary_data)
        new_row = _process_one_row(destination_config, source_row, primary_data.keys(), all_extracted_data, function_registry, source_file_name)
        output_rows.append(new_row)
    
    return pd.DataFrame(output_rows)

def _process_one_row(destination_config, source_row, source_columns, all_extracted_data, function_registry, source_file_name):
    """
    (æœ€ç»ˆç‰ˆ) è´Ÿè´£å¤„ç†å•è¡Œæ•°æ®çš„å®Œæ•´æ˜ å°„é€»è¾‘ã€‚
    """
    mappings = destination_config.get('mappings', [])
    standard_fields = {'source_file_name': source_file_name}
    
    extra_fields = source_row.get('extra_data', {})
    if not isinstance(extra_fields, dict):
        extra_fields = {}

    for rule in mappings:
        target_field = rule['target_field']
        raw_value = None
        
        if 'static_value' in rule:
            raw_value = rule['static_value']
        elif 'lookup_source' in rule:
            lookup_data = all_extracted_data.get(rule['lookup_source'])
            is_lookup_data_valid = False
            if isinstance(lookup_data, dict) and lookup_data:
                is_lookup_data_valid = True
            elif isinstance(lookup_data, pd.DataFrame) and not lookup_data.empty:
                is_lookup_data_valid = True
            if is_lookup_data_valid:
                lookup_row = pd.Series(lookup_data) if isinstance(lookup_data, dict) else lookup_data.iloc[0]
                field_name = find_field_from_aliases(lookup_row.index, rule.get('source_field_aliases', []))
                if field_name:
                    raw_value = lookup_row.get(field_name)
        else:
            field_name = find_field_from_aliases(source_columns, rule.get('source_field_aliases', []))
            if field_name:
                raw_value = source_row.get(field_name)

        transformed_value = raw_value
        for trans in rule.get('transformations', []):
            func = function_registry.get(trans['function'])
            if func:
                transformed_value = func(transformed_value)

        if rule.get('in_extra', False):
            extra_fields[target_field] = transformed_value
        else:
            standard_fields[target_field] = transformed_value
    
    if extra_fields:
        def _json_converter(o):
            if pd.isna(o):
                return None
            if isinstance(o, pd.Timestamp):
                return o.isoformat()
            if isinstance(o, Decimal):
                return str(o)
            if hasattr(o, 'item'):
                return o.item()
            raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")
        
        sanitized_extra_fields = {k: _sanitize_for_json(v) for k, v in extra_fields.items()}
        standard_fields['extra_data'] = json.dumps(sanitized_extra_fields, default=_json_converter, ensure_ascii=False)
    
    return standard_fields

def _sanitize_for_json(obj):
    """
    (å…¨æ–°çš„"æ·±åº¦æ¸…æ´"å·¥å…·)
    é€’å½’åœ°éå†ä¸€ä¸ªå­—å…¸æˆ–åˆ—è¡¨ï¼Œå°†æ‰€æœ‰Pandasçš„ç©ºå€¼(nan, NaTç­‰)éƒ½æ›¿æ¢æˆNoneã€‚
    """
    if isinstance(obj, dict):
        return {k: _sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_sanitize_for_json(elem) for elem in obj]
    elif pd.isna(obj):
        return None
    return obj
